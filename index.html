<!doctype html>
<html lang="en" prefix="og: http://ogp.me/ns#">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- SEO -->
    <title>Heba El-Shimy | Transformers 101</title>
    <meta name="description" content="PhD Viva Presentation">

    <!-- URL CANONICAL -->
     <link rel="canonical" href="https://www.macs.hw.ac.uk/~he4002/"> 

    <!-- Google Fonts -->
    <!--<link href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,700,700i%7CMaitree:200,300,400,600,700&amp;subset=latin-ext" rel="stylesheet">-->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Serif+Text:ital@0;1&family=Montserrat:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">

    <!-- CSS Base -->
    <link rel="stylesheet" type='text/css' media='all' href="css/webslides.css">

    <!-- CSS SVG Icons (Font Awesome) -->
    <link rel="stylesheet" type="text/css" media="all" href="css/svg-icons.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" type="text/css" media="all" href="css/custom.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
  </head>

  <body>
    <!-- <a href="#webslides" class="skip-link">Skip to main content</a> -->
    <header role="banner">
      <nav role="navigation">
        <!--<p class="logo">-->
          <a href="index.html" title="Front">
            <img class="logo" src="images/logo-light.png" alt="HWUD Logo">
            <img class="dark-logo" src="images/logo-dark.png" alt="HWUD Logo">
          </a>
        <!--</p>-->
      </nav>
    </header>

    <main role="main">
      <article id="webslides" class="montserrat-500">

        <!-- Quick Guide
          - Each parent <section> in the <article id="webslides"> element is an individual slide.
          - Vertical sliding = <article id="webslides" class="vertical">
          - <div class="wrap"> = container 90% / <div class="wrap size-50"> = 45%;
        -->

        <!-- Slide 1 -->
        <section id="section-1" data-section-title="Title">
          <span class="background" style="background-image:url('images/first.png'); background-size: cover; background-position: center center;"></span>
          <div class="wrap" style="margin-top: 8.5vh;">
            <h1 class="text-landing text-serif text-white text-shadow"><span style="text-decoration: underline; font-style: normal;">Transformers</span></h1>
            <p class="text-intro dm-serif-text-regular text-white text-shadow" style="text-transform: capitalize; margin-top: -1.2rem;">The Architecture Behind Modern AI</p>
            
            <!-- Presenter Information -->
            <div class="fadeInUp">
              <span class="text-white text-shadow text-slide-caption">Prepared &amp; presented by</span>
              <h3 class="dm-serif-text-regular text-white text-intro text-shadow">Heba El-Shimy</h3>
              <div style="margin-top: 1rem;" class="text-slide-caption">
                <span class="text-white text-shadow" style="display: block;">Email: <a href="mailto:H.Elshimy@hw.ac.uk" target="_blank" style="color: #ffffff; text-decoration: underline;">H.Elshimy@hw.ac.uk</a></span>
              </div>
              <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" style="display: inline-block; margin-top: 10px;">
                <img src="https://i.creativecommons.org/l/by/4.0/80x15.png" alt="Creative Commons Attribution 4.0 International License" style="height: clamp(12px, 1.5vw, 24px); width: auto;" />
              </a>
            </div> 
          </div>

          <!-- QR Code -->
          <div style="position: absolute; top: 20px; right: 20px; opacity: 0.65; font-size: 1rem;">
            <span class="text-white text-slide-caption" style="text-align: center; display: block; margin-bottom: 0.5rem;">Link to slides</span>
            <img src="images/Image.png" alt="QR code linking to presentation slides" style="width: 8rem;">
          </div>
        </section>

        <!-- Slide 2 -->
        <section id="section-2" class="bg-gradient-white" data-section-title="Previous Lecture">
          <div class="wrap">
            <div>
              <h3 class="dm-serif-text-regular slide-title">Last Lecture: <span>Text Processing &amp; RNNs</span></h3>
            </div>
            <hr class="hr">
            <div class="c-responsive">
              <ul>
                <li style="margin-bottom: 0.6rem;">Deep learning models cannot process raw text directly</li>
                <li style="margin-bottom: 0.6rem;">We need a way to convert text into a format that neural networks can process (numeric)</li>
                <li><span style="font-style: italic; text-decoration: underline;">Embedding:</span> is the process of representing discrete textual data into points in a continuous vector space</li>
              </ul>
              <img src="images/text-embeddings.png" class="aligncenter" alt="Image of the pipeline to process text for deep learning models by creating text embeddings" style="filter: drop-shadow(0px 4px 8px rgba(0, 0, 0, 0.15)); border-radius: 1rem; margin-top: 2rem; width: 70%;" />
              <p class="aligncenter text-slide-caption" style="color: #666; font-style: italic; margin-top: -3rem;">Source: Raschka, S. (2025) Build a large language model (from scratch). 1st edn. Manning Publications.</p>
            </div>
          </div>
        </section>

        <!-- Slide 3 -->
        <section id="section-3" class="bg-gradient-white" data-section-title="Previous Lecture">
          <div class="wrap">
            <h3 class="dm-serif-text-regular slide-title">Last Lecture: <span>Text Processing &amp; RNNs</span></h3>
            <hr class="hr">
            <div class="c-responsive" style="min-height: 60vh;">
                <ul>
                  <li style="margin-bottom: 1.8rem;">RNNs take the text embeddings as input and process them <span style="text-decoration: underline;">one at a time</span> in sequence</li>
                  <li>At each step, the RNN maintains a "hidden state" that acts as memory, storing information about all the words it has seen so far in the sequence</li>
                </ul>
              
              <!-- Right column: HTML Diagram (50%) -->
              <!-- <div class="size-50"> -->
              <!--   <iframe src="assets/rnn_text_processing_diagram.html"  -->
              <!--           class="rnn-diagram-iframe" scrolling="no"></iframe> -->
              <!--   <p class="aligncenter text-slide-caption" style="color: #666; font-style: italic; margin-top: 4vh;">Created by Claude AI</p> -->
              <!-- </div> -->
            </div>
          </div>
        </section>

        
        <!-- Slide 4 -->
        <section id="section-4" class="bg-gradient-white" data-section-title="Outline"> 
          <div class="wrap">
            <h3 class="dm-serif-text-regular slide-title">This Lecture: Outline</h3>
            <hr class="hr">
            <div class="toc" style="min-height: 60vh; margin-left: 2.4rem;">
              <ol>
                <li>
                  <a href="#slide=6" title="Go to LLMs">
                    <span class="chapter">LLMs</span>
                  </a>
                </li>
                <li>
                  <a href="#slide=8" title="Go to Transformers">
                    <span class="chapter">Transformers</span>
                  </a>
                </li>
                <li>
                  <a href="#slide=14" title="Go to Self-Attention Mechanism">
                    <span class="chapter">Self-Attention Mechanism</span>
                  </a>
                </li>
                <li>
                  <a href="#slide=17" title="Go to Recap">
                    <span class="chapter">Recap</span>
                  </a>
                </li>
                <li>
                  <a href="#slide=19" title="Go to Conclusions">
                    <span class="chapter">Conclusions</span>
                  </a>
                </li>
              </ol>
            </div>
            <!-- end .toc -->
            </div>
        </section>

        <!-- Slide 5 -->
        <section id="section-5" class="bg-gradient-white" data-section-title="Outline">
          <div class="wrap">
            <h3 class="dm-serif-text-regular slide-title">This Lecture: Learning Objectives</h3>
            <hr class="hr">
            <div class="c-responsive" style="min-height: 60vh;">
              <p>By the end of this lecture, you should be able to:</p>
                <ul>
                  <li style="margin-bottom: 1.8rem;">Understand the components of a transformer architecture</li>
                  <li style="margin-bottom: 1.8rem;">Explain why transformers are a significant advancement over RNNs and how they became the foundation of LLMs</li>
                  <li style="margin-bottom: 1.8rem;">Recognize the importance self-attention for transformer success</li>
                </ul>
            </div>
          </div>
        </section>

        <!-- Slide 6 -->
        <section id="section-6" class="bg-gradient-white" data-section-title="LLMs">
          <div class="wrap">
            <h2 class="dm-serif-text-regular slide-title">LLMs: What's Your Take?</h2>
            <hr class="hr">
            <div class="text-slide-body"> 
              <div style='position: relative; padding-bottom: 32%; padding-top: 0px; height: 0; overflow: hidden; width: 70%; margin: 0 auto;'><iframe sandbox='allow-scripts allow-same-origin allow-presentation' allowfullscreen='true' allowtransparency='true' frameborder='0' height='315' src='https://www.mentimeter.com/app/presentation/al47unrbwers2zbeam8pdmau7h34hs39/embed' style='position: absolute; top: 0; left: 0; width: 100%; height: 100%;' width='420'></iframe></div>
            </div>
          </div>
        </section>

        <!-- Slide 7 -->
        <section id="section-7" class="bg-gradient-white" data-section-title="LLMs">
          <div class="wrap">
            <h2 class="dm-serif-text-regular slide-title">Large Language Models (LLMs)</h2>
            <hr class="hr">
            <div class="text-slide-body"> 
              <p>An LLM is a deep neural network designed to understand, generate, and respond to human-like text. These models are trained on massive amounts of text data, sometimes encompassing large portions of the entire available text on the internet.</p>
              <img src="images/ai.png" class="aligncenter size-60" alt="Hierarchical depiction of the relationship between the different fields if AI, LLMs represent a specific application of deep learning techniques, using their ability to process and generate human-like text." style="filter: drop-shadow(0px 4px 8px rgba(0, 0, 0, 0.15)); border-radius: 1rem; margin-top: -2.0rem;" />
              <p class="aligncenter text-slide-caption" style="color: #666; font-style: italic; margin-top: -2.4em;">Source: Raschka, S. (2025) Build a large language model (from scratch). 1st edn. Manning Publications.</p>
            </div>
          </div>
        </section>

        <!-- Slide 8 -->
        <!-- <section id="section-8" class="bg-gradient-white slide-top" data-section-title="LLMs"> -->
        <!--   <div class="wrap" style="margin: 1rem 0 0 5rem;"> -->
        <!--     <h2 class="dm-serif-text-regular slide-title">Creating an LLM</h2><span class="text-intro">Two Stages</span> -->
        <!--     <hr> -->
        <!--     <div class="text-slide-body">  -->
        <!--       <ol style="width: 95%;">     -->
        <!--         <li style="margin-bottom: 1.8rem;"><span style="font-style: italic; text-decoration: underline;">Pretraining:</span> training on a large, diverse dataset to develop a broad understanding of language and create a "<span style="font-style: italic;">foundation model</span>"</li> -->
        <!--         <li><span style="font-style: italic; text-decoration: underline;">Fine-tuning:</span> training on a narrower dataset that is more specific to particular tasks or domains (could involve instructions)</li> -->
        <!--       </ol> -->
        <!--       <img src="images/llm.png" class="aligncenter" alt="Pretraining an LLM involves next-word prediction on large text datasets. A pretrained LLM can then be fine-tuned using a smaller labeled dataset." style="filter: drop-shadow(0px 4px 8px rgba(0, 0, 0, 0.15)); border-radius: 1rem; width: 40%; margin-top: -0.4rem;" /> -->
        <!--       <p class="aligncenter text-slide-caption" style="color: #666; font-style: italic; margin-top: -2.4em;">Source: Raschka, S. (2025) Build a large language model (from scratch). 1st edn. Manning Publications.</p> -->
        <!--     </div> -->
        <!--   </div> -->
        <!-- </section> -->

        <!-- Slide 8 -->
        <section id="section-8" class="bg-gradient-white" data-section-title="Transformers">
          <div class="wrap">
            <h3 class="dm-serif-text-regular slide-title">Transformers: <span class="text-intro">The Building Blocks of LLMs</span></h3>
            <hr class="hr">
            <div class="text-slide-body" style="display: flex; align-items: flex-start; gap: 2rem;">
              <div class="size-60">
                <p>A typical transformer consists of two main components:</p>
                <ul style="margin-top: -1.4em;">
                  <li style="margin-bottom: 1.2rem;"><em style="text-decoration: underline;">Encoder:</em> processes the input text; encoding it into a series of vectors capturing the contextual information of the input</li>
                  <li style="margin-bottom: 0.6rem;"><em style="text-decoration: underline;">Decoder:</em> takes the encoded vectors and generates the output text</li>
                <p>In each, the encoder and the decoder, there exists multiple transformer "blocks"; each block consisting of one self-attention and one feed forward (MLP) layer</p>
                </ul>
              </div>

              <!-- Right column: HTML Diagram (50%) -->
              <div class="size-40"> 
                <div id="flip-container" style="width: 75%; margin: 10% 0 0 12.5%; perspective: 1000px; cursor: pointer;">
                  <div id="flip-card" style="position: relative; width: 100%; height: 425px; text-align: center; transition: transform 0.6s; transform-style: preserve-3d;">
                    <div style="position: absolute; width: 115%; height: 115%; backface-visibility: hidden; left: -2.5rem;">
                      <img src="images/simple-transformer.png" alt="Simple Transformer diagram" style="width: 100%; height: 100%; object-fit: contain; filter: drop-shadow(0px 4px 8px rgba(0, 0, 0, 0.15)); margin-top: -2.5rem;" />
                    </div>
                    <div style="position: absolute; width: 112.5%; height: 112.5%; backface-visibility: hidden; transform: rotateY(180deg) translateX(40px);">
                      <img src="images/transformer.png" alt="Transformer architecture" style="width: 100%; height: 100%; object-fit: contain; filter: drop-shadow(0px 4px 8px rgba(0, 0, 0, 0.15)); margin-top: -2.5rem;" />
                    </div>
                  </div>
                </div>
                <div class="flip-card-caption">
                  <p class="aligncenter text-slide-caption">Click to flip the card for detailed architecture</p>
                  <p class="aligncenter text-slide-caption-small" style="margin-top: -4vh; line-height: 1.2;">Sources: 1. Claude AI. <br/> 2. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I., 2017. Attention is all you need. <em>Advances in neural information processing systems</em>, 30.</p>
                </div>
              </div>
            </div>
          </div>
        </section>

        <!-- Slide 10 -->
        <!-- <section id="section-10" class="bg-gradient-white slide-top" data-section-title="Transformers"> -->
        <!--   <div class="wrap" style="margin: 1rem 0 0 5rem;"> -->
        <!--     <h2 class="dm-serif-text-regular slide-title">Transformers</h2><span class="text-intro">Different Flavors</span> -->
        <!--     <hr> -->
        <!--     <div class="text-slide-body"> -->
        <!--       <div style="width: 100%; display: flex; justify-content: center;"> -->
        <!--         <ul class="flexblock gallery" style="max-height: 70vh; overflow: hidden; margin: 0; padding: 0; justify-content: center; background: none;"> -->
        <!--           <li style="height: 400px; background: white; width: 27%;"> -->
        <!--             <a href="#" style="background: white;"> -->
        <!--               <figure style="background: white;"> -->
        <!--                 <img alt="Diagram showing encoder-decoder transformer architecture with bidirectional encoder processing input and autoregressive decoder generating output, used for translation tasks" src="images/encoder-decoder.png" style="height: 280px; object-fit: contain;"> -->
        <!--                 <figcaption style="background: white;"> -->
        <!--                   <h2 style="font-size: 2rem;">Encoder-Decoder</h2> -->
        <!--                   <p style="font-size: 1.6rem;">Translation</p> -->
        <!--                 </figcaption> -->
        <!--               </figure> -->
        <!--             </a> -->
        <!--           </li> -->
        <!--           <li style="height: 400px; background: white; width: 29%;"> -->
        <!--             <a href="#" style="background: white;"> -->
        <!--               <figure style="background: white;"> -->
        <!--                 <img alt="Diagram showing encoder-only transformer architecture (BERT-style) with bidirectional processing for text understanding and classification tasks" src="images/encoder-only.png" style="height: 280px; object-fit: contain;"> -->
        <!--                 <figcaption style="background: white;"> -->
        <!--                   <h2 style="font-size: 2rem;">Encoder-only (BERT)</h2> -->
        <!--                   <p style="font-size: 1.6rem;">Text understanding</p> -->
        <!--                 </figcaption> -->
        <!--               </figure> -->
        <!--             </a> -->
        <!--           </li> -->
        <!--           <li style="height: 400px; background: white; width: 27%;"> -->
        <!--             <a href="#" style="background: white;"> -->
        <!--               <figure style="background: white;"> -->
        <!--                 <img alt="Diagram showing decoder-only transformer architecture (GPT-style) with causal attention for autoregressive text generation" src="images/decoder-only.png" style="height: 280px; object-fit: contain;"> -->
        <!--                 <figcaption style="background: white;"> -->
        <!--                   <h2 style="font-size: 2rem;">Decoder-only (GPT)</h2> -->
        <!--                   <p style="font-size: 1.6rem;">Text generation</p> -->
        <!--                 </figcaption> -->
        <!--               </figure> -->
        <!--             </a> -->
        <!--           </li> -->
        <!--         </ul> -->
        <!--       </div> -->
        <!--       <p class="aligncenter" style="font-size: 1.2rem; color: #666; font-style: italic;">Source: Raschka, S. (2025). <em>Build a large language model (from scratch)</em> [First edition].</p> -->
        <!--     </div> -->
        <!--   </div> -->
        <!-- </section> -->

        <!-- Slide 9 -->
        <section id="section-9" class="bg-gradient-white" data-section-title="Transformers">
          <div class="wrap">
            <h3 class="dm-serif-text-regular slide-title">Transformers: <span class="text-intro">As Building Blocks of LLMs</span></h3>
            <hr class="hr">
            <div style="display: flex; align-items: flex-start; gap: 2rem;">
              <div class="text-slide-body size-60">
                <ul> 
                    <li style="margin-bottom: 1.4rem;">LLMs stack multiple transformer blocks, each containing self-attention and feed-forward components</li>
                    <li style="margin-bottom: 1.4rem;"><span style="font-style: italic;">The word <b>Large</b> (in <b style="text-decoration: underline;">L</b>LMs)</span> refers to the model scale: billions of parameters from stacked transformer layers, trained on trillions of tokens</li> 
                    <!-- <li><p style="margin-bottom: -0.4rem; font-size: 75%; margin-top: -1.6rem;">Examples:</p> -->
                    <!--   <ul style="font-size: 75%;"> -->
                    <!--     <li style="margin-bottom: -0.2rem;">GPT-4: ~120 layers (estimated), 1.7T parameters (estimated)</li> -->
                    <!--     <li>LLaMA-65B: 80 layers, 65B parameters</li> -->
                    <!--     <li>DeepSeek: 61 layers, 671B parameters</li> -->
                    <!--   </ul> -->
                    <!-- </li> -->
                </ul>
              </div>
              <div class="size-40">
                <img src="images/block.png" alt="Illustration of a transformer block containing self-attention and feed-forward components" class="aligncenter" style="filter: drop-shadow(0px 4px 8px rgba(0, 0, 0, 0.15)); border-radius: 1rem;" />
                <p class="aligncenter text-slide-caption" style="color: #666; font-style: italic; margin-top: 4vh;">Source: Alammar, J. (2018) 'The Illustrated Transformer', available at: https://jalammar.github.io/illustrated-transformer</p>
              </div>
            </div>
          </div>
        </section>

        <!-- Slide 10 -->
        <section id="section-10" class="bg-gradient-white" data-section-title="Transformers">
          <div class="wrap">
            <h3 class="dm-serif-text-regular slide-title">Transformers: <span class="text-intro">What Makes Them Special?</span></h3>
            <hr class="hr">
            <div class="text-slide-body">
              <p style="margin-bottom: 1.8rem;">A quick look back at RNNs</p>
              <video src="assets/RNNAnimation.mp4" class="aligncenter size-50" autoplay loop controls muted style="filter: drop-shadow(0px 4px 8px rgba(0, 0, 0, 0.15)); border-radius: 1rem; margin-top: 1rem; max-width: 100%; height: auto; display: block; margin-left: auto; margin-right: auto;">
                Your browser does not support the video tag.
              </video>
              <p class="aligncenter text-slide-caption" style="color: #666; font-style: italic; margin-top: 0.8rem;">Source: animation created by Claude AI using Manim library; based on example from Raschka, S. (2025) Build a large language model (from scratch). 1st edn. Manning Publications.</p> 
            </div>
          </div>
        </section>

        <!-- Slide 11 -->
        <section id="section-11" class="bg-gradient-white" data-section-title="Transformers">
          <div class="wrap">
            <h3 class="dm-serif-text-regular slide-title">Transformers: <span class="text-intro">What Makes Them Special?</span></h3>
            <hr class="hr">
            <div class="text-slide-body">
              <video src="assets/TransformerAnimation.mp4" class="aligncenter size-60" autoplay loop controls muted style="filter: drop-shadow(0px 4px 8px rgba(0, 0, 0, 0.15)); border-radius: 1rem; margin-top: 1rem; max-width: 100%; height: auto; display: block; margin-left: auto; margin-right: auto;">
                Your browser does not support the video tag.
              </video>
              <p class="aligncenter text-slide-caption" style="color: #666; font-style: italic; margin-top: 0.8rem;">Source: animation created by Claude AI using Manim library; based on example from Raschka, S. (2025) Build a large language model (from scratch). 1st edn. Manning Publications.</p> 
            </div>
          </div>
        </section>

        <!-- Slide 12 -->
        <section id="section-12" class="bg-gradient-white" data-section-title="Transformers">
          <div class="wrap">
            <h3 class="dm-serif-text-regular slide-title">Quiz</h3>
            <hr class="hr">
            <div class="text-slide-body"> 
              <div style='position: relative; padding-bottom: 32%; padding-top: 0px; height: 0; overflow: hidden; width: 70%; margin: 0 auto;'><iframe sandbox='allow-scripts allow-same-origin allow-presentation' allowfullscreen='true' allowtransparency='true' frameborder='0' height='315' src='https://www.mentimeter.com/app/presentation/algjapus7uodsqgtkarr4xg8jjwht5b9/embed' style='position: absolute; top: 0; left: 0; width: 100%; height: 100%;' width='320'></iframe></div>
            </div>
          </div>
        </section>

        <!-- Slide 13 -->
        <section id="section-13" class="bg-gradient-white" data-section-title="Self-Attention Mechanism">
          <div class="wrap">
            <h3 class="dm-serif-text-regular slide-title">Attention is All You Need!</h3>
            <hr class="hr">
            <div class="text-slide-body">  
              <ul> 
                  <li style="margin-bottom: 1.8rem;">Self-attention serves as the cornerstone of the transformer architecture and is the reason for its success, and subsequently, the success of LLMs</li>
                  <li>It learns the relationships and dependencies between various parts of the input, such as words in a sentence or pixels in an image.</li>
              </ul>

              <!-- Three.js Self-Attention Visualization Container -->
              <div style="width: 100%; max-width: 700px; margin: 1rem auto; height: 220px; 
                          border-radius: 15px; box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);">
                <div id="self-attention-viz" style="width: 100%; height: 220px; margin: 0 auto; 
                                        position: relative; overflow: hidden; display: block;">
                  <div style="position: absolute; top: 15px; left: 20px; font-size: 16px; color: #444; 
                              font-weight: 600; z-index: 10;"> 
                  </div>
                </div>
              </div>

              <p class="aligncenter text-slide-caption" style="color: #666; font-style: italic; line-height: 1.4;">Read the paper: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I., 2017. Attention is all you need. <em>Advances in neural information processing systems</em>, 30. Available at: <a href="https://arxiv.org/pdf/1706.03762" target="_blank">https://arxiv.org/pdf/1706.03762</a></p> 
            </div>
          </div>
        </section>

        <!-- Slide 14 -->
        <section id="section-14" class="bg-gradient-white" data-section-title="Self-Attention Mechanism">
          <div class="wrap">
            <div class="text-slide-body" style="height: 70vh;">  
              <!-- Self-Attention Visualization -->
              <iframe 
                src="https://hebaelshimy-self-attention-demo.hf.space?__theme=light"
                width="95%"
                height="100%"
                frameborder="0"
                style="border-radius: 10px; box-shadow: 0 4px 20px rgba(0,0,0,0.1); margin: -5vh auto 0 auto;">
              </iframe> 
            </div>
          </div>
        </section>

        <!-- Slide 15 -->
        <section id="section-15" class="bg-gradient-white" data-section-title="Self-Attention Mechanism">
          <div class="wrap">
            <h3 class="dm-serif-text-regular slide-title">Quiz</h3>
            <hr class="hr">
            <div class="text-slide-body"> 
              <div style='position: relative; padding-bottom: 32%; padding-top: 0px; height: 0; overflow: hidden; width: 70%; margin: 0 auto;'><iframe sandbox='allow-scripts allow-same-origin allow-presentation' allowfullscreen='true' allowtransparency='true' frameborder='0' height='315' src='https://www.mentimeter.com/app/presentation/al1ht4wshpo2iyeuvnzc8jfvpyx7tqks/embed' style='position: absolute; top: 0; left: 0; width: 100%; height: 100%;' width='320'></iframe></div>
            </div>
          </div>
        </section>

        <!-- Slide 16 -->
        <section id="section-16" class="bg-gradient-white" data-section-title="Recap">
          <div class="wrap">
            <h3 class="dm-serif-text-regular slide-title">Recap</h3>
            <hr class="hr">
            <div class="text-slide-body">
              <ul>
                <li style="margin-bottom: 1.4rem;">Transformers revolutionized NLP by enabling parallel processing, solving RNN sequential limitations</li>
                <li style="margin-bottom: 1.4rem;">Self-attention mechanism allows models to learn relationships between all input tokens simultaneously</li>
                <li>LLMs are built on transformers, stacking multiple layers to capture complex language patterns</li>
              </ul>
            </div>

            <!-- Textbook Reference -->
            <div style="margin-left: 2.4rem; margin-top: 2.4rem; position: relative; width: 95%;">
              <div>
                <span style="font-size: 1.2rem;">Suggested reading</span>
                <h5 class="dm-serif-text-regular" style="margin: 0.5rem 0; font-size: 1.6rem; line-height: 1.8rem;">Raschka, S. (2025) Build a large language model (from scratch). 1st edn. Manning Publications.</h5>
                <p style="margin: 0.5rem 0; font-size: 1.2rem;">Available on HWU Discovery: <a href="https://discovery.hw.ac.uk" target="_blank" style="text-decoration: underline;">discovery.hw.ac.uk</a></p>
              </div>
              <img src="images/book.png" alt="Book cover of Build a large language model (from scratch)" style="width: 8rem; height: auto; border-radius: 0.2rem; filter: drop-shadow(0px 4px 8px rgba(0, 0, 0, 0.15)); position: absolute; right: 0; top: 0;">
          </div>
        </section>

        <!-- Slide 17 -->
        <section id="section-17" class="bg-gradient-white" data-section-title="Recap">
          <div class="wrap">
            <h3 class="dm-serif-text-regular slide-title">Next Lecture</h3>
            <hr class="hr">
            <div class="c-responsive" style="min-height: 60vh;">
              <ul>
                <li style="margin-bottom: 1.8rem;">Tokenization: Converting text into tokens for transformer model input</li>
                <li style="margin-bottom: 1.8rem;">Positional Encoding: Adding position information to token embeddings</li>
                <li>Multi-Head Attention: Enhancing self-attention by capturing diverse relationships</li>
              </ul>
            </div>

   </div>
        </section>

<!-- Slide 18 -->
        <section id="section-18" data-section-title="Conclusions">
          <span class="background" style="background-image:url('images/first.png');"></span>
          <div class="wrap">
            <h1 class="text-landing text-serif text-white size-60">Thank You. <br/>Questions?</h1>
          </div>
        </section>

    </main>
    <!--main-->

    <!-- Progress Bar -->
    <div class="progress-bar">
      <div class="progress-indicator"></div>
      <div class="progress-label">
        <div class="progress-section"></div>
      </div>
    </div>

    <footer>
      <div class="wrap text-slide-caption" style="margin-bottom: -1.5rem;">
        <p class="montserrat-400 text-shadow">
        <a class="link-underline" href="https://www.macs.hw.ac.uk/~he4002/">Heba El-Shimy</a> &copy; July 2025
        </p>
      </div>
    </footer>

    <!-- Required -->
    <script src="js/webslides.js"></script>

    <script>
      window.ws = new WebSlides();
    </script>
    
    <script src="js/progress-bar.js"></script>
    <script src="js/logo.js"></script>
    
    <!-- Flip card script -->
    <script>
      document.addEventListener('DOMContentLoaded', function() {
        const flipContainer = document.querySelector('#flip-container');
        const flipCard = document.querySelector('#flip-card');
        let isFlipped = false;
        
        if (flipContainer && flipCard) {
          flipContainer.addEventListener('click', function() {
            isFlipped = !isFlipped;
            flipCard.style.transform = isFlipped ? 'rotateY(180deg)' : 'rotateY(0deg)';
          });
        }
      });
    </script>

    <!-- OPTIONAL - svg-icons.js (fontastic.me - Font Awesome as svg icons) -->
    <script defer src="js/svg-icons.js"></script>
    
    <!-- Self-Attention Visualization -->
    <script src="js/self-attention-viz.js"></script>



  </body>
</html>
